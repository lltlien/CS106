{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9012462b-b872-4358-aec2-5fa2e0cd4f63"
      },
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "7d3a396f-9800-42ba-eb97-620cf9fe7357"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "e5a31a73-544b-44f6-aee4-9a5d60a7bb93"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "22b559b6-6393-4e98-a03c-fc227fa07473"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "d5a7759e-aaf3-4ca1-8797-5c040fb1f7fb"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "5439c193-6889-4786-9b79-a8aa857117e5"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play(env, policy_1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "2b9ddd86-b9fb-42f2-8f05-e9c58bad2d02"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "122ee67d-a8e8-4866-c45a-e394e5a446ed"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "1095715d-3b4e-4f48-f09c-5487839f538f"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "25c8acbc-5bdf-497f-fd43-0d4ffa5b46da"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 57/1000\n",
            "Average number of steps: 10.982456140350877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "6e0252fd-16c4-44a4-a555-b160651ca1bc"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 122/1000\n",
            "Average number of steps: 16.049180327868854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "843a13c1-7920-4135-80fc-000c969ab2a7"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 734/1000\n",
            "Average number of steps: 38.80926430517711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "9e2a8542-46e9-4dd8-ce84-517af2ab8166"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "251b118b-5a7d-46ea-ad49-89d81c33d4c4"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "2ca0422b-3950-47e8-8596-d268574621c5"
      },
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "05e9f90d-f67e-4f68-c22b-76fabc15cbd2"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "19fc997f-8794-4e6d-99a3-43d3ac062e59"
      },
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYYFq6BEXDd",
        "outputId": "48182cbb-3393-4720-b9b3-4413bea7c9a3"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 80-th iteration.\n",
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEfU3NYE7xN",
        "outputId": "1c6b4746-da6c-4b9a-fdef-e552a417fd83"
      },
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            \n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "7579b43a-c5d6-4e11-d282-a4745b044806"
      },
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "27b39258-a84c-4974-8091-163df3b461f0"
      },
      "source": [
        "optimal_v_values"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGCF4G7XErH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99486c7-db13-4d13-b686-0f11aaf37908"
      },
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-9e926206ddb9>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  policy = np.zeros(env.observation_space.n, dtype=np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "984ed2e2-3922-48eb-9a11-023440fe4acb"
      },
      "source": [
        "optimal_policy\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 728/1000\n",
            "Average number of steps: 37.41483516483517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZY-vM9YXMRX",
        "outputId": "6bf02fc9-8cc9-456a-90df-c0d1fa604d75"
      },
      "source": [
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 56/1000\n",
            "Average number of steps: 11.946428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcYV5xbSZAHe"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POLICY ITERATION"
      ],
      "metadata": {
        "id": "IbaatyWbWcfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(env, policy, gamma=0.9, max_iters=500):\n",
        "    # Khởi tạo giá trị của tất cả các trạng thái là 0\n",
        "    values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for _ in range(max_iters):\n",
        "        prev_values = np.copy(values)\n",
        "\n",
        "        # Cập nhật giá trị của mỗi trạng thái\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "            q_value = 0\n",
        "\n",
        "            # Tính giá trị q cho từng hành động trong mỗi trạng thái\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_values[next_state])\n",
        "\n",
        "            values[state] = q_value\n",
        "\n",
        "        # Kiểm tra sự hội tụ\n",
        "        if np.all(np.isclose(values, prev_values)):\n",
        "            break\n",
        "\n",
        "    return values"
      ],
      "metadata": {
        "id": "NpuWCVCBYQyA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(env, v_values, policy, gamma=0.9):\n",
        "    # Khởi tạo\n",
        "    v_values_new = np.copy(v_values)\n",
        "    policy_new = np.copy(policy)\n",
        "    num_iterations = 0\n",
        "\n",
        "    # Lặp qua mỗi trạng thái trong môi trường\n",
        "    for i in range(1000):\n",
        "        policy_temp = np.copy(policy_new)\n",
        "        \n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Lặp qua mỗi hành động\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "\n",
        "                # Lặp qua mỗi kết quả có thể xảy ra\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * v_values_new[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # Chọn hành động tốt nhất và cập nhật v_values_new[state]\n",
        "            best_action = np.argmax(q_values)\n",
        "            policy_temp[state] = best_action\n",
        "            v_values_new[state] = np.max(q_values)\n",
        "\n",
        "        # Trả về policy mới nếu policy không thay đổi\n",
        "        if np.all(policy_new == policy_temp):\n",
        "            break\n",
        "\n",
        "        # Cập nhật policy nếu có thay đổi\n",
        "        policy_new = np.copy(policy_temp)\n",
        "\n",
        "    return policy_new\n"
      ],
      "metadata": {
        "id": "SVWCZJeNYu0e"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy_FrozenLake_v0 = np.random.randint(0,3,env.observation_space.n)"
      ],
      "metadata": {
        "id": "GvgtZof9YzEM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FrozenLake-v1\n"
      ],
      "metadata": {
        "id": "P3b4t7AwYEkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration - FrozenLake-v1\n",
        "env = gym.make('FrozenLake-v1')\n",
        "start_time = time.time()\n",
        "# Khởi tạo chính sách ngẫu nhiên\n",
        "policy = np.random.randint(0,3, env.observation_space.n)\n",
        "# Đánh giá giá trị của chính sách hiện tại\n",
        "values = evaluate_policy(env, policy)\n",
        "# Cải thiện chính sách\n",
        "optimal_policy_FrozenLake_v0 = policy_improvement(env, values, policy,gamma=0.9)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "ndxMBln5Yz1v"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration frozenlake-v1\n",
        "start_time_vi =time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "optimal_policy_FrozenLake_v0_2 = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "end_time_vi =time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUPC2BWyaNbU",
        "outputId": "c6b97aea-8323-42a0-d92f-9a4ead65905b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-9e926206ddb9>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  policy = np.zeros(env.observation_space.n, dtype=np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env, optimal_policy_FrozenLake_v0 , 1000)\n",
        "play_multiple_times(env, optimal_policy_FrozenLake_v0_2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Y45ryjarqW",
        "outputId": "e48721b0-fdb1-40e7-8837-96d3e48399d8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 424/1000\n",
            "Average number of steps: 28.830188679245282\n",
            "Number of successes: 760/1000\n",
            "Average number of steps: 36.39605263157895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FrozenLack8x8-v1"
      ],
      "metadata": {
        "id": "rDI8TU3_YEsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration - FrozenLake-v1\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "start_time8x8 = time.time()\n",
        "# Khởi tạo chính sách ngẫu nhiên\n",
        "policy8x8 = np.random.randint(0,3, env.observation_space.n)\n",
        "# Đánh giá giá trị của chính sách hiện tại\n",
        "values8x8 = evaluate_policy(env, policy8x8)\n",
        "# Cải thiện chính sách\n",
        "optimal_policy_FrozenLake8x8_v0 = policy_improvement(env, values8x8, policy8x8,gamma=0.9)\n",
        "end_time8x8 = time.time()"
      ],
      "metadata": {
        "id": "8jsxo-whYRST"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration frozenlake-v1\n",
        "start_time_vi8x8 =time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "optimal_policy_FrozenLake8x8_v0_2 = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "end_time_vi8x8 =time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlLqJh3obkiu",
        "outputId": "5221fa45-6a10-47ab-9c6b-05a92ef5c595"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 117-th iteration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-9e926206ddb9>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  policy = np.zeros(env.observation_space.n, dtype=np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env, optimal_policy_FrozenLake8x8_v0, 1000)\n",
        "play_multiple_times(env, optimal_policy_FrozenLake8x8_v0_2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ2x8qsBb50i",
        "outputId": "dd1f3105-43ec-44a0-9843-69192cd66e96"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 598/1000\n",
            "Average number of steps: 69.876254180602\n",
            "Number of successes: 728/1000\n",
            "Average number of steps: 73.52747252747253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Taxi-v3"
      ],
      "metadata": {
        "id": "EkW3MrkwYE2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration taxi-v3\n",
        "env = gym.make('Taxi-v3')\n",
        "start_time_taxi =time.time()\n",
        "policy_Taxi_v3 = np.random.randint(0,3,env.observation_space.n)\n",
        "v_values_Taxi_v3 = evaluate_policy(env, policy_Taxi_v3)\n",
        "optimal_policy_Taxi_v3 = policy_improvement(env, v_values_Taxi_v3,policy_Taxi_v3, gamma=0.9)\n",
        "end_time_taxi =time.time()\n",
        "\n",
        "# Value Iteration taxi-v3\n",
        "start_time_vi_taxi =time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "optimal_policy_Taxi_v3_2 = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "end_time_vi_taxi =time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX88PWXVWg43",
        "outputId": "cfdff571-5a6d-4dc4-efc9-ca9413cdebbb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 116-th iteration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-9e926206ddb9>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  policy = np.zeros(env.observation_space.n, dtype=np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env, optimal_policy_Taxi_v3, 1000)\n",
        "play_multiple_times(env, optimal_policy_Taxi_v3_2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rdu8aJLcrJt",
        "outputId": "f11d9f17-59f1-4f37-9457-499212e80c97"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.11\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So sánh Policy Iteration & Value Iteration"
      ],
      "metadata": {
        "id": "1uvcqYvmdfqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Độ phức tạp:\n",
        "\n",
        "* Value Iteration có độ phức tạp thời gian cao hơn Policy Iteration trong mỗi vòng lặp do phải duyệt qua tất cả các hành động tại mỗi trạng thái. Độ phức tạp của Value Iteration là O(S^2 * A), trong đó S là số lượng trạng thái và A là số lượng hành động.\n",
        "* Policy Iteration có độ phức tạp thời gian thấp hơn Value Iteration trong mỗi vòng lặp, vì chỉ tính toán giá trị dựa trên một random policy và sau đó cải thiện policy. Độ phức tạp của Policy Iteration là O(S^2), trong đó S là số lượng trạng thái."
      ],
      "metadata": {
        "id": "nnHjlTUpdmkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Iteration - frozenlake-v1 :\",end_time-start_time)\n",
        "print(\"Policy Iteration - frozenlake-v1:\",end_time_vi-start_time_vi)\n",
        "print(\"Value Iteration - frozenlake8x8-v1 :\",end_time8x8-start_time8x8)\n",
        "print(\"Policy Iteration - frozenlake8x8-v1:\",end_time_vi8x8-start_time_vi8x8)\n",
        "print(\"Value Iteration - taxi-v3 :\",end_time_taxi-start_time_taxi)\n",
        "print(\"Policy Iteration - taxi-v3:\",end_time_vi_taxi-start_time_vi_taxi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPLhp7Kgdlx7",
        "outputId": "19861829-46c7-46ab-b89a-1389e6fa6cc5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration - frozenlake-v0 : 0.010804176330566406\n",
            "Policy Iteration - frozenlake-v0: 0.06967616081237793\n",
            "Value Iteration - frozenlake8x8-v0 : 0.04332709312438965\n",
            "Policy Iteration - frozenlake8x8-v0: 0.3626523017883301\n",
            "Value Iteration - taxi-v3 : 0.4876139163970947\n",
            "Policy Iteration - taxi-v3: 2.326550245285034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lý thuyết cho thấy PI có độ phức tạp thời gian cho từng vòng lặp nhỏ hơn so với VI. Tuy nhiên, kết quả thực nghiệm cho thấy PI mất nhiều thời gian hơn để cho ra kết quả so với VI.\n",
        "* VI nhanh hơn so với PI rất nhiều ( gấp 4 - 8 lần )\n",
        "* Ở Taxi-v3 PI thực sự mất nhiều thời gian ( 2.32655... )\n"
      ],
      "metadata": {
        "id": "suuG2Dbzfhgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Có thể suy ra rằng, việc đánh giá và cải thiện chiến lược cho từng lần lặp của PI tốn nhiều thời gian hơn so với quá trình tìm giá trị tối ưu trong VI, dẫn đến PI mất nhiều thời gian hơn trong thực nghiệm."
      ],
      "metadata": {
        "id": "jTBTy5o2jDgE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOSrI3O3gPhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}